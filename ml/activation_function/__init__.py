"""
为什么叫“激活函数 Activation Function”?
我们先从“激活”这个词说起。
想象一个神经网络像人脑一样有很多“神经元”：
每个神经元都会接收输入信号；
然后决定要不要“被激活”——也就是要不要对这个输入“作出反应”。
激活函数就是这个神经元的“反应规则”。

比如：
如果输入太小（没什么用的信息）→ 不激活（输出 0）
如果输入够强（有代表性的信号）→ 激活（输出一个值）

它相当于给每个神经元装上一个“判断器”，
让它自己决定：“我这次要不要工作？”

所以叫 “激活函数”，因为它控制了神经元的“激活”与“关闭”。

简而言之，激活函数让每个神经元学会“自己决定什么时候说话、什么时候闭嘴”。
不同输入触发不同的“说话组合”，网络就能对不同样本“关注不同的特征”，从而具备识别、理解、分类的能力。

如果没有激活函数（全是线性的），整个网络就相当于一个 大号线性方程。再多层都没用，结果仍是“直线”或“平面”的映射。
激活函数让网络能画出弯弯曲曲的决策边界，也就是能“理解复杂现象”。


常见激活函数的性格与用途：
| 名称             | 公式 & 图像特点                 | 性格（大白话）                | 常见用途                     |
| -------------- | ----------------------------- | ---------------------------- | ------------------------     |
| Sigmoid       | S 形曲线，0~1 之间              | 温和、有点迟钝，容易“饱和”       | 老式模型（逻辑回归、简单二分类）  |
| Tanh          | -1~1 对称的 S 形               | 比 Sigmoid 稍灵敏点，居中       | RNN（循环网络）中偶尔用         |
| ReLU          | max(0, x)                     | 直爽、干脆，负数直接拉黑         | CNN、MLP、Transformer 中的主角 |
| Leaky ReLU    | 正常 ReLU，但负数区也留一点泄露   | 比 ReLU 稍微柔和，防止“死神经元” | 替代 ReLU 的常用变体            |
| PReLU         | Leaky ReLU 的改进版，泄露程度可学 | 自适应能力更强                 | 高级 CNN 模型                |
| ELU / GELU    | 平滑版 ReLU，有轻微弧度          | 聪明、有温度的 ReLU            | Transformer、BERT 等大型模型  |
| Softmax       | 把多个值变成概率分布（总和=1）     | 投票型函数，让结果变成“可能性”   | 输出层（分类任务）              |

这些激活函数看起来都只是几条简单的线或曲线，它们真能让神经网络变得‘聪明’到能识别图像、理解语言吗？
答案是：是的，它们真的能。
而且它们的“强大”不是来自函数本身有多复杂，而是来自——“简单非线性 + 层层叠加” 产生了指数级复杂性。

每层的 ReLU/Tanh 虽然很简单，但层与层之间的复合，会产生高度非线性的映射。
数学上，如果每层都是非线性函数 σ，哪怕 σ 只是 ReLU（分段线性），整个函数 f(x) 就能拼出成千上万段不同的线性区域。

想象你要画一条复杂曲线。你手上只有几条小折线（ReLU），你不能画圆，但可以拼无数小折线去逼近任何曲线。
激活函数的威力就在于：它是“拼图的弯折点”，决定了网络能拼出多少样的形状。

所以，激活函数看起来简单，但它是深度学习的非线性引擎。它让神经网络从“线性方程堆”变成“复杂模式工厂”，让机器能看图识物、理解语言、下围棋。
"""