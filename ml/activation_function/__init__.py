"""
为什么叫“激活函数 Activation Function”?
我们先从“激活”这个词说起。
想象一个神经网络像人脑一样有很多“神经元”：
每个神经元都会接收输入信号；
然后决定要不要“被激活”——也就是要不要对这个输入“作出反应”。
激活函数就是这个神经元的“反应规则”。

比如：
如果输入太小（没什么用的信息）→ 不激活（输出 0）
如果输入够强（有代表性的信号）→ 激活（输出一个值）

它相当于给每个神经元装上一个“判断器”，
让它自己决定：“我这次要不要工作？”

所以叫 “激活函数”，因为它控制了神经元的“激活”与“关闭”。

简而言之，激活函数让每个神经元学会“自己决定什么时候说话、什么时候闭嘴”。
不同输入触发不同的“说话组合”，网络就能对不同样本“关注不同的特征”，从而具备识别、理解、分类的能力。

如果没有激活函数（全是线性的），整个网络就相当于一个 大号线性方程。再多层都没用，结果仍是“直线”或“平面”的映射。
激活函数让网络能画出弯弯曲曲的决策边界，也就是能“理解复杂现象”。


常见激活函数的性格与用途：
| 名称             | 公式 & 图像特点                 | 性格（大白话）                | 常见用途                     |
| -------------- | ----------------------------- | ---------------------------- | ------------------------     |
| Sigmoid       | S 形曲线，0~1 之间              | 温和、有点迟钝，容易“饱和”       | 老式模型（逻辑回归、简单二分类）  |
| Tanh          | -1~1 对称的 S 形               | 比 Sigmoid 稍灵敏点，居中       | RNN（循环网络）中偶尔用         |
| ReLU          | max(0, x)                     | 直爽、干脆，负数直接拉黑         | CNN、MLP、Transformer 中的主角 |
| Leaky ReLU    | 正常 ReLU，但负数区也留一点泄露   | 比 ReLU 稍微柔和，防止“死神经元” | 替代 ReLU 的常用变体            |
| PReLU         | Leaky ReLU 的改进版，泄露程度可学 | 自适应能力更强                 | 高级 CNN 模型                |
| ELU / GELU    | 平滑版 ReLU，有轻微弧度          | 聪明、有温度的 ReLU            | Transformer、BERT 等大型模型  |
| Softmax       | 把多个值变成概率分布（总和=1）     | 投票型函数，让结果变成“可能性”   | 输出层（分类任务）              |

"""