# encoding=utf-8
"""
超迷你版“按前缀续写句子”，最古老的n-gram算法

有点像最原始的“马尔可夫链文本生成”，不过是用字符级来做的：
我们关心的东西是：“给定前缀 text，下一字符是什么”
这个代码的做法就是在语料里统计：
在历史上出现过多少次 “textX” 这样的片段（X 为一个字符）
然后把所有这些 X 拿出来，从中随机抽一个作为当前的“下一字符”

这就是一个 非常简化的“基于前缀的马尔可夫生成”：
状态：当前前缀 text
可能的下一状态：text + 某个字符
转移概率：由语料中出现的频次隐性决定（因为随机从样本里采）

专业名字可以叫：
- 字符级 n-gram 模型（这里 n = 当前前缀长度 + 1）
- 简单马尔可夫链文本生成
不过这里没有显式算概率，而是直接从所有出现过的样本中均匀随机选一个。
"""
import random

corpus = [
    "我喜欢学习", "我会跑步", "我想吃苹果",
    "你喜欢做题", "你想吃草莓", "你会打羽毛球",
    "他喜欢读书", "他会游泳", "他想吃香蕉",
    "她喜欢上课", "她会打乒乓球", "她想喝牛奶",
    "我们喜欢背单词", "我们想吃葡萄", "我们会踢足球",
    "大家喜欢写字", "大家会打篮球", "大家想吃西瓜",
    "哥哥会爬山", "哥哥想喝可乐",
    "姐姐会骑车", "姐姐想喝茶",
    "朋友喜欢听讲", "朋友会滑冰", "朋友想喝咖啡",
    "同学会滑雪", "同学想喝果汁",
    "老师喜欢考试",
    "学生喜欢提问",
    "孩子喜欢进步"]

prefix = "我"
n = 3

text = prefix
for _ in range(n):
    # 所有候选“下一个字符”的列表
    opts = []
    for s in corpus:
        # 在句子 s 中滑动窗口，找 “text” 出现的位置
        for i in range(len(s) - len(text)):
            if s[i:i + len(text)] == text:
                opts.append(s[i + len(text)])
    if not opts: break
    print(f"opts = {opts}")
    text += random.choice(opts)

print(text)
